# -*- coding: utf-8 -*-
"""Amazon_Review_With_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p4bLG7TtQZIzORAstqieKYplOyLhDwPy
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# from google.colab import drive
# from collections import Counter
# drive.mount('/content/drive')

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from sklearn.feature_extraction.text import CountVectorizer

TRAIN_POSITIVE_SAMPLE_DIR = "/content/drive/MyDrive/Assignments_ML_With_Graphs/hw1-handout/data/train/positive/"
TRAIN_NEGATIVE_SAMPLE_DIR = "/content/drive/MyDrive/Assignments_ML_With_Graphs/hw1-handout/data/train/negative/"

TEST_POSITIVE_SAMPLE_DIR = "/content/drive/MyDrive/Assignments_ML_With_Graphs/hw1-handout/data/test/positive/"
TEST_NEGATIVE_SAMPLE_DIR = "/content/drive/MyDrive/Assignments_ML_With_Graphs/hw1-handout/data/test/negative/"

def load_text_files(directory):
    text_data = []
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
            text_data.append(text)
    return text_data

# Load train data
train_positive_data = load_text_files(TRAIN_POSITIVE_SAMPLE_DIR)
train_negative_data = load_text_files(TRAIN_NEGATIVE_SAMPLE_DIR)

# Load test data
test_positive_data = load_text_files(TEST_POSITIVE_SAMPLE_DIR)
test_negative_data = load_text_files(TEST_NEGATIVE_SAMPLE_DIR)

all_train_data = train_positive_data + train_negative_data

UNIQUE_WORDS = {}
train_data = train_positive_data + train_negative_data

for sentence in train_data:
  for word in sentence.split(" "):
      if word in UNIQUE_WORDS:
          UNIQUE_WORDS[word] += 1
      else:
          UNIQUE_WORDS[word] = 0

FREQUENT_10K_words = Counter(UNIQUE_WORDS)

FREQUENT_10K_words = FREQUENT_10K_words.most_common()[:10000]
FREQUENT_10K_words = [word[0] for word in FREQUENT_10K_words]

from torch.utils.data import Dataset
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

tokenizer = get_tokenizer("basic_english")

def yield_tokens(data_iter):
    for text in data_iter:
        yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(FREQUENT_10K_words), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

vocab["are"]

class SentimentDataset(Dataset):
    def __init__(self, texts, labels, vocab):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab

    def text_to_indices(self, text):
        return [self.vocab[word] for word in text.split()]

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        indices = self.text_to_indices(text)

        return {'text': indices, 'label': label}

def custom_collate(batch):
    texts = [torch.tensor(item['text'], dtype=torch.long) for item in batch]
    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
    texts = pad_sequence(texts, batch_first=True, padding_value=0)

    return {'text': texts, 'label': labels}

# LSTM model
class SentimentLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(SentimentLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.lstm(embedded)
        output = self.fc(lstm_out[:, -1, :])
        output = self.softmax(output)
        return output

# # LSTM model
# class SentimentLSTMWithEmbeddings(nn.Module):
#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
#         super(SentimentLSTMWithEmbeddings, self).__init__()
#         weights = (torch.rand(10001, 100) * 2) - 1

#         for word in embedding_dict.keys():
#             if word in vocab:
#                 word_index = vocab[word]
#                 weights[word_index] = torch.tensor(embedding_dict[word])
#         self.embedding = nn.Embedding.from_pretrained(weights, freeze=True)

#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
#         self.fc = nn.Linear(hidden_dim, output_dim)
#         self.softmax = nn.LogSoftmax(dim=1)

#     def forward(self, x):
#         embedded = self.embedding(x)
#         lstm_out, _ = self.lstm(embedded)
#         output = self.fc(lstm_out[:, -1, :])
#         output = self.softmax(output)
#         return output

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_dataset = SentimentDataset(all_train_data, [1] * len(train_positive_data) + [0] * len(train_negative_data), vocab=vocab)
test_dataset = SentimentDataset(test_positive_data + test_negative_data, [1] * len(test_positive_data) + [0] * len(test_negative_data), vocab=vocab)

train_loader = DataLoader(train_dataset, batch_size=2, collate_fn=custom_collate, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=2, collate_fn=custom_collate)

from torch.optim import lr_scheduler
model = SentimentLSTM(vocab_size=len(vocab), embedding_dim=100, hidden_dim=100, output_dim=2).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)
scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)

import matplotlib.pyplot as plt
# Training loop
def train_model(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct_train = 0
    total_train = 0

    for batch in train_loader:
        inputs, labels = batch['text'].to(device), batch['label'].to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

    average_loss = total_loss / len(train_loader)
    accuracy_train = correct_train / total_train
    return average_loss, accuracy_train

# Testing loop
def test_model(model, test_loader, device):
    model.eval()
    correct_test = 0
    total_test = 0
    total_loss_test = 0

    with torch.no_grad():
        for batch in test_loader:
            inputs, labels = batch['text'].to(device), batch['label'].to(device)
            # Skip empty sequences
            if inputs.size(1) == 0:
                continue

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss_test += loss.item()

            # Calculate testing accuracy
            _, predicted = torch.max(outputs.data, 1)
            total_test += labels.size(0)
            correct_test += (predicted == labels).sum().item()

    accuracy_test = correct_test / total_test
    average_loss_test = total_loss_test / len(test_loader)
    return average_loss_test, accuracy_test

# Plot function
def plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies):
    plt.figure(figsize=(12, 4))

    # Plot Loss
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train')
    plt.plot(test_losses, label='Test')
    plt.title('Training and Testing Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Plot Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(train_accuracies, label='Train')
    plt.plot(test_accuracies, label='Test')
    plt.title('Training and Testing Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

import time

epochs = 50
train_losses = []
test_losses = []
train_accuracies = []
test_accuracies = []

# Record start time
start_time = time.time()

for epoch in range(epochs):
    train_loss, train_accuracy = train_model(model, train_loader, optimizer, criterion, device)
    test_loss, test_accuracy = test_model(model, test_loader, device)

    train_losses.append(train_loss)
    test_losses.append(test_loss)
    train_accuracies.append(train_accuracy)
    test_accuracies.append(test_accuracy)
    # Update the scheduler
    scheduler.step()

    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, '
          f'Train Accuracy: {train_accuracy * 100:.2f}%, Test Accuracy: {test_accuracy * 100:.2f}%')

end_time = time.time()

total_time = end_time - start_time
print(f'Total time taken for all epochs: {total_time:.2f} seconds')

# Plot the training and testing metrics
plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies)

# Calculate statistics for the training data

total_unique_words = len(UNIQUE_WORDS)
total_training_examples = len(all_train_data)
total_positive_examples = len(train_positive_data)
total_negative_examples = len(train_negative_data)

# Calculate the ratio of positive to negative examples
ratio_positive_to_negative = total_positive_examples / total_negative_examples

# Calculate the average length of documents
average_length_of_documents = sum(len(doc.split()) for doc in all_train_data) / total_training_examples

# Calculate the max length of documents
max_length_of_documents = max(len(doc.split()) for doc in all_train_data)

# Print the statistics
print(f'Total Unique Words in Training: {total_unique_words}')
print(f'Total Training Examples: {total_training_examples}')
print(f'Total Positive Examples: {total_positive_examples}')
print(f'Total Negative Examples: {total_negative_examples}')
print(f'Ratio of Positive to Negative Examples: {ratio_positive_to_negative:.2f}')
print(f'Average Length of Document in Training: {average_length_of_documents:.2f} words')
print(f'Max Length of Document in Training: {max_length_of_documents} words')

